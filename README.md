# ISABELLE: Offline Vision Assistant for the Blind

**Built for the Google - Gemmaâ€¯3n Impact Challenge 2025**

---

## ğŸ“Œ Project Overview

ISABELLE is a voice-activated AI assistant for the visually impaired. Activated by saying:

> â€œIsabelle, what is in front?â€

It captures the scene, runs the **Gemmaâ€¯3n E4B-it-int4** LLM via MediaPipe Tasks entirely on-device, and speaks out a description using built-in TTSâ€”all **offline** and **private**.

---

## âœ… Key Feature

- ğŸ¯ Real-time object/scene description (â€œwhat is in front?â€)
- âœ… Tested live with a blind individual at a local NGO
- âœ… Runs fully offline on **Google Pixelâ€¯8â€¯Pro**

---

## ğŸ—ï¸ Technical Architecture

- **Frontend**: Flutter (`main.dart`, `camera_service.dart`)
- **Android Native**: Kotlin (`Gemma3nProcessor.kt`) using MediaPipe Tasks
- **Model**: `Gemma_3n_E4B-it-int4.task` (hosted on GCS, not included in repo)
- **TTS**: Offline Text-to-Speech
- **Privacy**: No network calls, telemetry disabled

---

## ğŸ§  Prompt Logic

Prompt template sent to the model:

> You are ISABELLE, a blind-assistive AI assistant running on a phone. Briefly describe what is in front of the user based on the image.

---

## ğŸ“¦ Installation

To build the app:

```bash
flutter pub get
flutter build apk --release
